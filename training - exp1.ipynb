{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import sys\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPISODES = 1000\n",
    "weights_file = 'model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0   # exploration rate\n",
    "        self.epsilon_min = 0.01 # minimum exploration rate\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, rand = True):\n",
    "        if np.random.rand() <= self.epsilon and rand:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            self.model.fit(state, target_f, epochs=1, verbose = 0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model = load_model(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "agent.load(\"CartPole-agent.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000, score: 21, e: 1.0\n",
      "episode: 10/1000, score: 14, e: 0.96\n",
      "episode: 20/1000, score: 18, e: 0.91\n",
      "episode: 30/1000, score: 18, e: 0.86\n",
      "episode: 40/1000, score: 12, e: 0.82\n",
      "episode: 50/1000, score: 42, e: 0.78\n",
      "episode: 60/1000, score: 19, e: 0.74\n",
      "episode: 70/1000, score: 12, e: 0.71\n",
      "episode: 80/1000, score: 95, e: 0.67\n",
      "episode: 90/1000, score: 37, e: 0.64\n",
      "episode: 100/1000, score: 11, e: 0.61\n",
      "episode: 110/1000, score: 16, e: 0.58\n",
      "episode: 120/1000, score: 22, e: 0.55\n",
      "episode: 130/1000, score: 56, e: 0.52\n",
      "episode: 140/1000, score: 29, e: 0.5\n",
      "episode: 150/1000, score: 27, e: 0.47\n",
      "episode: 160/1000, score: 11, e: 0.45\n",
      "episode: 170/1000, score: 31, e: 0.43\n",
      "episode: 180/1000, score: 16, e: 0.41\n",
      "episode: 190/1000, score: 18, e: 0.39\n",
      "episode: 200/1000, score: 16, e: 0.37\n",
      "episode: 210/1000, score: 31, e: 0.35\n",
      "episode: 220/1000, score: 52, e: 0.33\n",
      "episode: 230/1000, score: 25, e: 0.32\n",
      "episode: 240/1000, score: 35, e: 0.3\n",
      "episode: 250/1000, score: 31, e: 0.29\n",
      "episode: 260/1000, score: 54, e: 0.27\n",
      "episode: 270/1000, score: 26, e: 0.26\n",
      "episode: 280/1000, score: 56, e: 0.25\n",
      "episode: 290/1000, score: 73, e: 0.23\n",
      "episode: 300/1000, score: 39, e: 0.22\n",
      "episode: 310/1000, score: 106, e: 0.21\n",
      "episode: 320/1000, score: 75, e: 0.2\n",
      "episode: 330/1000, score: 36, e: 0.19\n",
      "episode: 340/1000, score: 159, e: 0.18\n",
      "episode: 350/1000, score: 196, e: 0.17\n",
      "episode: 360/1000, score: 147, e: 0.17\n",
      "episode: 370/1000, score: 132, e: 0.16\n",
      "episode: 380/1000, score: 93, e: 0.15\n",
      "episode: 390/1000, score: 128, e: 0.14\n",
      "episode: 400/1000, score: 300, e: 0.14\n",
      "episode: 410/1000, score: 305, e: 0.13\n",
      "episode: 420/1000, score: 180, e: 0.12\n",
      "episode: 430/1000, score: 177, e: 0.12\n",
      "episode: 440/1000, score: 345, e: 0.11\n",
      "episode: 450/1000, score: 499, e: 0.11\n",
      "episode: 460/1000, score: 107, e: 0.1\n",
      "episode: 470/1000, score: 135, e: 0.095\n",
      "episode: 480/1000, score: 140, e: 0.091\n",
      "episode: 490/1000, score: 285, e: 0.086\n",
      "episode: 500/1000, score: 392, e: 0.082\n",
      "episode: 510/1000, score: 137, e: 0.078\n",
      "episode: 520/1000, score: 296, e: 0.074\n",
      "episode: 530/1000, score: 196, e: 0.071\n",
      "episode: 540/1000, score: 334, e: 0.067\n",
      "episode: 550/1000, score: 138, e: 0.064\n",
      "episode: 560/1000, score: 151, e: 0.061\n",
      "episode: 570/1000, score: 143, e: 0.058\n",
      "episode: 580/1000, score: 102, e: 0.055\n",
      "episode: 590/1000, score: 148, e: 0.052\n",
      "episode: 600/1000, score: 122, e: 0.05\n",
      "episode: 610/1000, score: 152, e: 0.047\n",
      "episode: 620/1000, score: 130, e: 0.045\n",
      "episode: 630/1000, score: 499, e: 0.043\n",
      "episode: 640/1000, score: 118, e: 0.041\n",
      "episode: 650/1000, score: 97, e: 0.039\n",
      "episode: 660/1000, score: 358, e: 0.037\n",
      "episode: 670/1000, score: 73, e: 0.035\n",
      "episode: 680/1000, score: 113, e: 0.033\n",
      "episode: 690/1000, score: 126, e: 0.032\n",
      "episode: 700/1000, score: 63, e: 0.03\n",
      "episode: 710/1000, score: 134, e: 0.029\n",
      "episode: 720/1000, score: 118, e: 0.027\n",
      "episode: 730/1000, score: 137, e: 0.026\n",
      "episode: 740/1000, score: 409, e: 0.025\n",
      "episode: 750/1000, score: 135, e: 0.023\n",
      "episode: 760/1000, score: 77, e: 0.022\n",
      "episode: 770/1000, score: 91, e: 0.021\n",
      "episode: 780/1000, score: 154, e: 0.02\n",
      "episode: 790/1000, score: 152, e: 0.019\n",
      "episode: 800/1000, score: 101, e: 0.018\n",
      "episode: 810/1000, score: 71, e: 0.017\n",
      "episode: 820/1000, score: 48, e: 0.016\n",
      "episode: 830/1000, score: 37, e: 0.016\n",
      "episode: 840/1000, score: 115, e: 0.015\n",
      "episode: 850/1000, score: 41, e: 0.014\n",
      "episode: 860/1000, score: 151, e: 0.013\n",
      "episode: 870/1000, score: 22, e: 0.013\n",
      "episode: 880/1000, score: 94, e: 0.012\n",
      "episode: 890/1000, score: 100, e: 0.012\n",
      "episode: 900/1000, score: 125, e: 0.011\n",
      "episode: 910/1000, score: 111, e: 0.01\n",
      "episode: 920/1000, score: 116, e: 0.01\n",
      "episode: 930/1000, score: 152, e: 0.01\n",
      "episode: 940/1000, score: 94, e: 0.01\n",
      "episode: 950/1000, score: 126, e: 0.01\n",
      "episode: 960/1000, score: 113, e: 0.01\n",
      "episode: 970/1000, score: 157, e: 0.01\n",
      "episode: 980/1000, score: 70, e: 0.01\n",
      "episode: 990/1000, score: 147, e: 0.01\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "batch_size = 32\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(500):\n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            if e % 10 == 0:\n",
    "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                      .format(e, EPISODES, time, agent.epsilon))\n",
    "            break\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 300 timesteps\n",
      "Episode finished after 319 timesteps\n",
      "Episode finished after 154 timesteps\n",
      "Episode finished after 123 timesteps\n",
      "Episode finished after 155 timesteps\n",
      "Episode finished after 131 timesteps\n",
      "Episode finished after 205 timesteps\n",
      "Episode finished after 201 timesteps\n",
      "Episode finished after 258 timesteps\n",
      "Episode finished after 241 timesteps\n",
      "Episode finished after 250 timesteps\n",
      "Episode finished after 288 timesteps\n",
      "Episode finished after 176 timesteps\n",
      "Episode finished after 137 timesteps\n",
      "Episode finished after 151 timesteps\n",
      "Episode finished after 227 timesteps\n",
      "Episode finished after 211 timesteps\n",
      "Episode finished after 175 timesteps\n",
      "Episode finished after 277 timesteps\n",
      "Episode finished after 195 timesteps\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(20):\n",
    "    state = env.reset()\n",
    "    for t in range(1000):\n",
    "        env.render()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        action = agent.act(state, rand=False)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.save('CartPole-agent.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
